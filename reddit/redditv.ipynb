{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "##from pytube import YouTube\n",
    "import praw\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path # For writing videos into the data folder\n",
    "\n",
    "# Make sure the praw.ini file is in the correct directory\n",
    "# Also, make sure there is a folder called \"data\" in the same directory that contains this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_streamable(url):\n",
    "    \"\"\"\n",
    "    Convert streamable url from reddit to actual video url\n",
    "    \n",
    "    Arguments:\n",
    "    url | string | the streamable link that reddit gives you\n",
    "    \n",
    "    Returns:\n",
    "    video_url | string | the link to the actual video\n",
    "    \"\"\"\n",
    "    \n",
    "    # Replace streamable.com with api.streamable.com/videos to get the api link\n",
    "    api_url = re.sub(string = url,\n",
    "                     pattern = 'streamable\\.com/',\n",
    "                     repl = 'api.streamable.com/videos/')\n",
    "    # The api link gets us a json file, we use json() to decode it\n",
    "    json1 = requests.get(api_url).json()\n",
    "    # Access the video url in the json file\n",
    "    video_url = json1['files']['mp4']['url']\n",
    "    \n",
    "    return video_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_reddit(video_url):\n",
    "    \"\"\"\n",
    "    Convert v.redd.it video url into audio url\n",
    "    \n",
    "    Arguments:\n",
    "    video_url | string | the fallback video url from reddit's secure media\n",
    "    \n",
    "    Returns:\n",
    "    audio_url | string | the audio url inferred from the fallback video url; this can be obtained\n",
    "                       | in the DASHPlaylist.mpd file, search for the base url and audio sections\n",
    "    \"\"\"\n",
    "    audio_url = \"\"\n",
    "    \n",
    "    if bool( re.search(string = video_url,\n",
    "                       pattern = \"\\/DASH_\\d{2,4}\\.mp4\") ):    \n",
    "        # If there is a .mp4 extension in the url, replace with \"DASH_audio.mp4\"\n",
    "        audio_url = re.sub(string = video_url,\n",
    "                           pattern = \"\\/DASH_\\d{2,4}\\.mp4\",\n",
    "                           repl = \"/DASH_audio.mp4\")\n",
    "    \n",
    "    elif bool ( re.search(string = video_url,\n",
    "                          pattern = \"\\/DASH_(\\d{2,4})\\?\") ):\n",
    "        # If there is no .mp4 extension in the url, just replace with \"audio\"\n",
    "        audio_url = re.sub(string = video_url,\n",
    "                           pattern = \"\\/DASH_(\\d{2,4})\\?\",\n",
    "                           repl = \"/audio?\")\n",
    "    \n",
    "    else:\n",
    "        print(\"error, no match detected\")\n",
    "\n",
    "    return(audio_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_url_from_submission(submission):\n",
    "    \"\"\"\n",
    "    Get the video file urls from the given submission\n",
    "    \n",
    "    Arguments:\n",
    "    submission | praw.models.reddit.submission.Submission | the elements in the listing generator\n",
    "               | you get these when you iterate through a listing generator\n",
    "               | Ex. for i in reddit.subreddit(\"PublicFreakout\").top(limit = 3):\n",
    "    \n",
    "    Returns:\n",
    "    video_url | string array | array that contains the links to the relevant files\n",
    "                             | if the link is streamable, it will be a list of size 1\n",
    "                             | linking to the video because the video contains audio\n",
    "                             | if the link is reddit, it will be a list of size 2\n",
    "                             | with the first element linking to the video graphics\n",
    "                             | and the second element linking to the audio\n",
    "    \n",
    "    Example conversions:\n",
    "    https://streamable.com/u2jzoo into https://api.streamable.com/videos/u2jzoo,\n",
    "    then retrieve url from json\n",
    "    \n",
    "    For reddit, get subreddit post, find the fall back video url\n",
    "    and the audio url by replacing stuff\n",
    "    Example: \n",
    "    https://v.redd.it/9v2san14was51/DASH_720.mp4?source=fallback to\n",
    "    https://v.redd.it/9v2san14was51/DASH_audio.mp4?source=fallback\n",
    "    \n",
    "    https://v.redd.it/w56rwny74y351/DASH_360?source=fallback to\n",
    "    https://v.redd.it/w56rwny74y351/audio?source=fallback\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the string array\n",
    "    url_array = []\n",
    "    # I'm using an array because Streamable gives us audio + video in one file\n",
    "    # but Reddit has two links, one for video (graphics) only and one for audio only\n",
    "    \n",
    "    # Check if it's streamable or reddit\n",
    "    # If streamable\n",
    "    if submission.domain == \"streamable.com\":\n",
    "        # Convert the streamable url to the actual video url\n",
    "        video_url = convert_streamable(submission.url)\n",
    "        # Append the url to the array\n",
    "        url_array.append(video_url)\n",
    "    \n",
    "    # If reddit\n",
    "    elif submission.domain == \"v.redd.it\":\n",
    "        try:\n",
    "            # Get the url from secure_media instead\n",
    "            video_url = submission.secure_media['reddit_video']['fallback_url']\n",
    "        except:\n",
    "            # Try causes an error if it's a crosspost. We have to access crosspost_parent_list,\n",
    "            # 0, secure_media, etc.\n",
    "            # Example:\n",
    "            # https://www.reddit.com/r/PublicFreakout/comments/hafl7q/cop_chokes_and_punches_teenage_girl_in_the_head.json\n",
    "            video_url = submission.crosspost_parent_list[0]['secure_media']['reddit_video']['fallback_url']\n",
    "            \n",
    "        # Append the video url to the list\n",
    "        url_array.append(video_url)\n",
    "        # Get the audio\n",
    "        audio_url = convert_reddit(video_url)\n",
    "        url_array.append(audio_url)\n",
    "        \n",
    "    else:\n",
    "        print(\"error\" + submission.domain)\n",
    "    \n",
    "    return url_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for download videos from subreddit urls\n",
    "def download_video(url, audio = False):\n",
    "    \"\"\"\n",
    "    Download a file from a url\n",
    "    \n",
    "    Arguments:\n",
    "    url | string | the link that you want to download from\n",
    "    audio | boolean | True will set the extension to .mp3, False will set the extension to .mp4\n",
    "    \n",
    "    Returns:\n",
    "    local_filename | string | name of the downloaded file\n",
    "    \"\"\"\n",
    "        \n",
    "    # Check the root website\n",
    "    root_url = url.split('/')[2]\n",
    "    video_url = \"\"\n",
    "    \n",
    "    # If this is reddit\n",
    "    if re.match(string = root_url, pattern = \"^v\\.redd\\.it\"):\n",
    "        if audio == True:\n",
    "            file_ext = \".mp3\"\n",
    "        else:\n",
    "            file_ext = \".mp4\"\n",
    "        # Use the second last part of the url within the slashes, it should be unique\n",
    "        local_filename = url.split('/')[-2]\n",
    "        # If the filename doesn't have the '.mp4' extension\n",
    "        if not bool( re.search( string = local_filename,\n",
    "                                pattern = \"\\.mp4$\") ):\n",
    "            local_filename += file_ext\n",
    "    \n",
    "    # If this is streamable, chop off the stuff after .mp4\n",
    "    elif re.match(string = root_url, pattern = \"^.*streamable.com\"):\n",
    "        # Use the second last part of th\n",
    "        local_filename = re.sub(string = url.split('/')[-1],\n",
    "                                pattern = \"(?<=\\.mp4).*\",\n",
    "                                repl = \"\")\n",
    "        \n",
    "    else:\n",
    "        # Create file name with the last part of the url within the slashes\n",
    "        local_filename = url.split('/')[-1]\n",
    "    \n",
    "    # Name of script\n",
    "    script_name = \"redditv.ipynb\"\n",
    "    path1 = Path(\"./\", script_name)\n",
    "    \n",
    "    # Create a path for the data folder\n",
    "    data_path = Path(\"./data\").resolve()\n",
    "    # If the data folder doesn't exist\n",
    "    if not data_path.is_dir():\n",
    "        # Create the data folder\n",
    "        data_path.mkdir()\n",
    "    \n",
    "    # Check that the script is a file\n",
    "    if path1.is_file():\n",
    "        # This is the absolute path we want our file to be written in\n",
    "        path2 = Path(path1.resolve().parent, \"data\")\n",
    "        # This is the file with the path that open will write to\n",
    "        path3 = Path(path2, local_filename)\n",
    "    \n",
    "    # Using with to automatically close the connection when we are done with it\n",
    "    with requests.get(url, stream = True) as req:\n",
    "        # Raise an http error if there is one\n",
    "        req.raise_for_status()\n",
    "        # Write the file in binary\n",
    "        with open(path3, 'wb') as video_file:\n",
    "            for chunk in req.iter_content(chunk_size = 4000): \n",
    "                # If you have chunk encoded response uncomment if\n",
    "                # and set chunk_size parameter to None.\n",
    "                #if chunk: \n",
    "                video_file.write(chunk)\n",
    "                \n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98147891230761\n"
     ]
    }
   ],
   "source": [
    "# Connect to Reddit's API\n",
    "reddit = praw.Reddit(\"bot1\", user_agent = \"bot1\")\n",
    "# Check that the praw.ini file worked\n",
    "print(reddit.user.me())\n",
    "# Link to json file with streamable and reddit links as of Oct. 13, 2020\n",
    "# https://www.reddit.com/r/PublicFreakout/top/.json?t=year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### # Select the public freakout subreddit\n",
    "### sr1 = reddit.subreddit(\"PublicFreakout\")\n",
    "### # Select the top 3 (it is selecting top 3 of the year by default)\n",
    "### top1 = sr1.top(limit = 3)\n",
    "### \n",
    "### # Checking the contents of the top posts in \"PublicFreakout\"\n",
    "### # Initialize arrays\n",
    "### urls = []\n",
    "### # Place the relevant info into the array from the selected top 3\n",
    "### for i in top1:\n",
    "###     urls.append(video_url_from_submission(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### # Downloading a reddit video graphics\n",
    "### download_video(urls[1][0], audio = False)\n",
    "### # Downloading a reddit video's audio\n",
    "### download_video(urls[1][1], audio = True)\n",
    "### # Downloading a streamable video\n",
    "### download_video(urls[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "errorgfycat.com\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "errorgfycat.com\n",
      "73\n",
      "74\n",
      "errori.redd.it\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "errorgfycat.com\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "errori.imgur.com\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "erroryoutube.com\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "urls appended\n",
      "Error with post number 23\n",
      "Error with post number 37\n",
      "Error with post number 39\n",
      "Error with post number 129\n",
      "Error with post number 135\n",
      "Error with post number 211\n",
      "Error with post number 230\n",
      "Error with post number 232\n",
      "files downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download a dataset of 10\n",
    "\n",
    "# Select the public freakout subreddit\n",
    "sr1 = reddit.subreddit(\"PublicFreakout\")\n",
    "# Select the top 10 (it is selecting top 10 of the year by default)\n",
    "top1 = sr1.top(limit = 250)\n",
    "# Initialize arrays\n",
    "urls = []\n",
    "### buggy1 = []\n",
    "# Place the relevant info into the array from the selected top 3\n",
    "for i in top1:\n",
    "    ### # Debugging error where some attribute doesn't exist\n",
    "    print(len(urls))\n",
    "    ### if(len(urls) == 15):\n",
    "    ###     buggy1.append(i)\n",
    "    try:\n",
    "        urls.append( video_url_from_submission(i) )\n",
    "    except:\n",
    "        print( urls.append( video_url_from_submission(i) ) )\n",
    "print(\"urls appended\")\n",
    "# Download the videos\n",
    "for i in range( len(urls) ):\n",
    "    for j in range( len(urls[i]) ):\n",
    "        if j == 0:\n",
    "            audio1 = False\n",
    "        else:\n",
    "            audio1 = True\n",
    "        try:\n",
    "            download_video(urls[i][j], audio = audio1)\n",
    "        except:\n",
    "            # Getting this HTTP error\n",
    "            # 403 Client Error: Forbidden for url: https://v.redd.it/716d4vdxcqu41/audio?source=fallback\n",
    "            # Also, some of the posts are not videos. When printing the length of the url, sometimes\n",
    "            # it says \"errorgfycat.com\" or \"errori.redd.it\" for example.\n",
    "            print(\"Error with post number \" + str(i))\n",
    "\n",
    "print(\"files downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://v.redd.it/716d4vdxcqu41/DASH_1080?source=fallback',\n",
       " 'https://v.redd.it/716d4vdxcqu41/audio?source=fallback']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is information; it is not directly used for downloading video/audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Submission(id='gtsaam'), Submission(id='gzvmsr'), Submission(id='gvdl01')]\n",
      "True\n",
      "[['https://cdn-cf-east.streamable.com/video/mp4/u2jzoo.mp4?Expires=1602904380&Signature=N3j3TOy0K7F9hAdVuliRJIIehHQCgapiNOwJ8Rffy2AGV3~jc8bjYTvcll~kUMdaMASw1htFNF2NZCpuyQFApnqZCk-arbMJM77JQlQwqWRrM6nwCOdCHzHtbKVB8CpP351Rndp8306q0x~isrQRoSh2BVfswSxmOp0WTfkVBRog~b34xKq0YedZe8aCiYfkbLVNtaIZb5h12HIwc~xVt1L6QnY0u3kh7ow7gstSbcvXapucOj7LkIXmniiWIgenTfF6yWTMlQlcI~~wjqgE2hPJdo8w29UpTH6OiV~vDByvIh7LU6AQ9hyhamDBoyUdXli4fLadS9ygRpfDFwqGMQ__&Key-Pair-Id=APKAIEYUVEN4EVB2OKEQ'], ['https://v.redd.it/w56rwny74y351/DASH_360?source=fallback', 'https://v.redd.it/w56rwny74y351/audio?source=fallback'], ['https://v.redd.it/q356rndwpj251/DASH_720?source=fallback', 'https://v.redd.it/q356rndwpj251/audio?source=fallback']]\n"
     ]
    }
   ],
   "source": [
    "### # This entire code chunk is commented out. To uncomment it easily, move the cursor between the\n",
    "### # third and fourth '#', after the space. Then, hold option and drag to the bottom of the\n",
    "### # code block; this should form a straight cursor line that spans all of the rows.\n",
    "### # delete the '#'. This works for re-commenting the code as well\n",
    "### \n",
    "### # Example of checking the type of a submission within the listing generator\n",
    "### # Initialize an list to store the submissions\n",
    "### check1 = []\n",
    "### # This creates a listing generator; this must be created every time you want to use top1\n",
    "### top1 = sr1.top(limit = 3)\n",
    "### # For through the submissions\n",
    "### for i in top1:\n",
    "###     # and append them to the initialized list above\n",
    "###     check1.append(i)\n",
    "### # Check that the intialized list not empty\n",
    "### print(check1)\n",
    "### # Example of checking the type of a reddit submission\n",
    "### print(type(check1[0]) == praw.models.reddit.submission.Submission)\n",
    "### \n",
    "### # Check the urls\n",
    "### print(urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redditv",
   "language": "python",
   "name": "redditv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
